{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778294c1-b55b-4eb2-afac-a3e377d005ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_google_vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f1d49-9d4d-4f52-b227-93d936391f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87901ff6-90ec-47fc-8391-7657983eecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d853a8e-5b1e-4e31-bab6-bf28cba5967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59dd3c-1bb9-46c4-8fcc-a6d6a0728e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf43c3-a413-4f25-90a1-311c7b1c24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-cypher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453b368-824c-4384-a024-6c154f2b1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b83d1-9862-4c9e-a65c-b0fb317361e6",
   "metadata": {},
   "source": [
    "## Neo4j Advanced rag \n",
    "https://github.com/langchain-ai/langchain/tree/master/templates/neo4j-advanced-rag this is has open ai reference and below you can find the gcp model implementation\n",
    "\n",
    "Future work\n",
    "\n",
    "Comparitive study with https://python.langchain.com/v0.1/docs/use_cases/graph/constructing/ approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39416959-a4c4-420a-bc5c-ec176669cc22",
   "metadata": {},
   "source": [
    "![Architecture](./Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1672176-b9cf-4ee3-88dd-20fd38e60bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from langchain_google_vertexai import (\n",
    "    VertexAI,\n",
    "    ChatVertexAI,\n",
    "    VertexAIEmbeddings,\n",
    "    VectorSearchVectorStore,\n",
    ")\n",
    "from google.cloud import aiplatform\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from neo4j.exceptions import ClientError\n",
    "import os\n",
    "from vertexai.generative_models import GenerativeModel, Image\n",
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "from langchain_google_vertexai import ChatVertexAI, HarmBlockThreshold, HarmCategory\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from typing import List, Tuple\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel,RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import base64\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "import typing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7cc935-e7fe-4146-9a7c-5daca7d6121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"bolt://url\"\n",
    "username =\"neo4j\"\n",
    "password = \"password\"\n",
    "os.environ[\"NEO4J_URI\"] = url\n",
    "os.environ[\"NEO4J_USERNAME\"] = username\n",
    "os.environ[\"NEO4J_PASSWORD\"] = password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7ca925-a023-422d-9d5b-8239c959e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"\"  # @param {type:\"string\"}\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b4e866-4e50-4842-89ad-f2a41533f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATCH (n) MATCH ()-[r]->() RETURN n, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19f6c6c-d83c-47ea-be81-8742a31fa694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATCH (n) OPTIONAL MATCH (n)-[r]-() DELETE n, r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2bae52-e73c-462a-a5c6-de3dcfd142ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"/home/path/rag/data/image\"\n",
    "pdf_file_name = \"/home/path/rag/data/filename.pdf\"\n",
    "Pdf_title=\"filename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be289937-764a-4193-971c-ad25e591ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 16:42:48.530937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 16:42:48.548337: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 16:42:48.553289: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-03 16:42:48.567324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "image_pdf_folder_path = \"/home/path/rag/data/images\"\n",
    "pdf_file_name = \"/home/path/rag/data/filename.pdf\"\n",
    "\n",
    "# Extract images, tables, and chunk text from a PDF file.\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=pdf_file_name,\n",
    "    extract_images_in_pdf=False,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=pdf_folder_path,\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    extract_image_block_output_dir=image_pdf_folder_path,\n",
    "    extract_image_block_to_payload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5b49d5-8f8c-4afa-9f7b-dacf064a88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Categorize extracted elements from a PDF into tables and texts.\n",
    "tables = []\n",
    "texts = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        tables.append(str(element))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        texts.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2515496b-f2b0-482c-8a4b-cfb374632a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=5000, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4fd0c8-66b3-43ab-b63b-9d578bb58fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemini-1.0-pro-vision\"\n",
    "\n",
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(\n",
    "    texts: List[str], tables: List[str], summarize_texts: bool = False\n",
    ") -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a elaborate concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "    empty_response = RunnableLambda(\n",
    "        lambda x: AIMessage(content=\"Error processing document\")\n",
    "    )\n",
    "    # Text summary chain\n",
    "    model = VertexAI(\n",
    "        temperature=0, model_name=MODEL_NAME, max_output_tokens=1024\n",
    "    ).with_fallbacks([empty_response])\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts:\n",
    "        if summarize_texts:\n",
    "            text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "        else:\n",
    "            text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91aeb4a9-a308-48a4-977d-c209ae1e4b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    model = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n",
    "\n",
    "    msg = model(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\n",
    "    If it's a table, extract all elements of the table.\n",
    "    If it's a graph, explain the findings in the graph.\n",
    "    Do not include any numbers that are not mentioned in the image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(image_pdf_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbfaa58d-369a-43a8-bcc3-7e17913bbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_all_texts = text_summaries + table_summaries + image_summaries \n",
    "joined_texts = \" \".join(combine_all_texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3deb260-f4ad-4f8d-b1d4-089c6e8d0217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_4k_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc7760-b2c5-47b6-941e-5d49ef2ad6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.create_documents(texts_4k_token)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5fcb845-4dd3-455c-a6ee-6295af056136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfdf277f-37cd-420a-9ba4-0393c8eb4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings & LLM models\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embedding_dimension = 768\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Loading Gemini Pro Vision Model\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")\n",
    "# Initializing embedding model\n",
    "EMBEDDING_MODEL =VertexAIEmbeddings(model_name=\"textembedding-gecko@003\", dimension=768) #TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "\n",
    "# Loading Gemini Pro Model\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-1.0-pro-vision\",\n",
    "    safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cae88430-a45e-4c2d-90fe-55def3b67651",
   "metadata": {},
   "source": [
    "# Ingest Parent-Child node pairs (old) : all strategies failed\n",
    "parent_splitter = TokenTextSplitter(chunk_size=3000, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=750, chunk_overlap=24)\n",
    "parent_documents = parent_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44225643-c77a-410b-a0f2-a36da68d3921",
   "metadata": {},
   "source": [
    "# (old) : parent strategies failed\n",
    "parent_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "parent_documents = parent_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6ac4378-ac81-4d98-ab6c-260bac9cf1f6",
   "metadata": {},
   "source": [
    "# Ingest Parent-Child node pairs #  :all  passed\n",
    "parent_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=256, chunk_overlap=24)\n",
    "parent_documents = parent_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bc019e4-8b47-45b2-8357-4a8269d2b819",
   "metadata": {},
   "source": [
    "# Ingest Parent-Child node pairs #  :all  passed\n",
    "parent_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=24)\n",
    "parent_documents = parent_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fbd4e90a-aad8-44d1-9629-4baad63fb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=24)\n",
    "parent_documents = parent_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2d185be8-9c11-4846-90e4-a204f8a3d6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parent_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "85b261c7-4890-4168-af06-930f19495e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ad5d19e0-9c69-4ba7-94f0-a4bb3c617101",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1cd64db4-ff27-4f6b-affd-206803df2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9473\n",
      "9559\n",
      "5097\n",
      "8972\n",
      "8430\n",
      "4056\n",
      "9175\n",
      "9317\n",
      "3676\n",
      "9110\n",
      "5388\n"
     ]
    }
   ],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    child_documents = child_splitter.split_documents([parent])\n",
    "    print(len(parent.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ce532-21e7-42ef-a695-12a6020f2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    child_documents = child_splitter.split_documents([parent])\n",
    "    print(len(parent.page_content))\n",
    "    params = {\n",
    "        \"parent_text\": parent.page_content,\n",
    "        \"parent_id\": i,\n",
    "        \"parent_embedding\": EMBEDDING_MODEL.embed_query(parent.page_content), #EMBEDDING_MODEL.get_embeddings([parent.page_content])[0].values,\n",
    "        \"children\": [\n",
    "            {\n",
    "                \"text\": c.page_content,\n",
    "                \"id\": f\"{i}-{ic}\",\n",
    "                \"embedding\": EMBEDDING_MODEL.embed_query(c.page_content) #EMBEDDING_MODEL.get_embeddings([c.page_content])[0].values,\n",
    "            }\n",
    "            for ic, c in enumerate(child_documents)\n",
    "        ],\n",
    "    }\n",
    "     # Ingest data\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    SET p.text = $parent_text\n",
    "    WITH p\n",
    "    CALL db.create.setVectorProperty(p, 'embedding', $parent_embedding)\n",
    "    YIELD node\n",
    "    WITH p \n",
    "    UNWIND $children AS child\n",
    "    MERGE (c:Child {id: child.id})\n",
    "    SET c.text = child.text\n",
    "    MERGE (c)<-[:HAS_CHILD]-(p)\n",
    "    WITH c, child\n",
    "    CALL db.create.setVectorProperty(c, 'embedding', child.embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index for child\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('parent_document', \"\n",
    "            \"'Child', 'embedding', 768, 'cosine')\",\n",
    "            {},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "    # Create vector index for parents\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('typical_rag', \"\n",
    "            \"'Parent', 'embedding', 768, 'cosine')\",\n",
    "            {},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "# CALL db.index.vector.createNodeIndex('parent_document', 'Child', 'embedding', 768, 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eac2a2-548d-4215-a225-3ab654f2e44b",
   "metadata": {},
   "source": [
    "![neo4j](./neo4j3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedff08c-ef06-4340-a035-384707d88064",
   "metadata": {},
   "source": [
    "![neo4j](./neo4j2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "bc0460c5-9c4e-4706-a4cf-5d2e694faacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatVertexAI(client=<google.cloud.aiplatform_v1beta1.services.prediction_service.client.PredictionServiceClient object at 0x7f4b0c18ed50>, project='edl-idaas-rnd-platform-d5ae', model_name='gemini-1.0-pro-vision', model_family=<GoogleModelFamily.GEMINI: '1'>, full_model_name='projects/edl-idaas-rnd-platform-d5ae/locations/us-central1/publishers/google/models/gemini-1.0-pro-vision', client_options=ClientOptions: {'api_endpoint': 'us-central1-aiplatform.googleapis.com', 'client_cert_source': None, 'client_encrypted_cert_source': None, 'quota_project_id': None, 'credentials_file': None, 'scopes': None, 'api_key': None, 'api_audience': None, 'universe_domain': None}, default_metadata=(), temperature=0.0)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae3d93-1375-4f82-869e-9fafe233f0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb7a4e-9f76-44fe-a66e-310b98b1caa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e8071fcc-20f7-48ab-90f4-05031c25308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest frequently asked questions  questions\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    \"\"\"Generating hypothetical questions about text.\"\"\"\n",
    "\n",
    "    questions: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Generated hypothetical questions based on  the information from the text\"\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e389c9c9-8cf0-4d01-a11c-96dbccbdf42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Use the given format to generate frequently asked questions from the \"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "question_chain = questions_prompt | llm.with_structured_output(Questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0d3b1808-a9da-40fc-9c84-ef382b7ca37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_chain = questions_prompt | llm.with_structured_output(Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d8ebcd4f-653f-4a18-85e1-c3b0e411967b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parent_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "43f19cd5-1e2e-430a-99d9-033bf51f4bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4a6e05c9-4129-4a05-b59f-25521ebac1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are the potential impacts of climate change on businesses?']\n",
      "-------\n",
      "[\"What are Emirates Islamic's environmental goals?\"]\n",
      "-------\n",
      "['What is the most popular sport among children?']\n",
      "-------\n",
      "['What are the 17 Sustainable Development Goals?']\n",
      "-------\n",
      "['What is the most popular sport among children?']\n",
      "-------\n",
      "['What is the percentage of operations assessed for risk related to corruption in 2023?']\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    resp = question_chain.invoke(parent.page_content)\n",
    "    if (resp != None):\n",
    "        questions = resp.questions\n",
    "        print(questions)\n",
    "        print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "acef9b53-0ef9-4cf2-8c67-1b4a0033baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    resp = question_chain.invoke([parent.page_content])\n",
    "    if (resp != None):\n",
    "        questions = resp.questions\n",
    "        params = {\n",
    "            \"parent_id\": i,\n",
    "            \"questions\": [\n",
    "                {\"text\": q, \"id\": f\"{i}-{iq}\", \"embedding\": EMBEDDING_MODEL.embed_query(q)} #EMBEDDING_MODEL.get_embeddings([q])[0].values}\n",
    "                for iq, q in enumerate(questions)\n",
    "                if q\n",
    "            ],\n",
    "        }\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "        MERGE (p:Parent {id: $parent_id})\n",
    "        WITH p\n",
    "        UNWIND $questions AS question\n",
    "        CREATE (q:Question {id: question.id})\n",
    "        SET q.text = question.text\n",
    "        MERGE (q)<-[:HAS_QUESTION]-(p)\n",
    "        WITH q, question\n",
    "        CALL db.create.setVectorProperty(q, 'embedding', question.embedding)\n",
    "        YIELD node\n",
    "        RETURN count(*)\n",
    "        \"\"\",\n",
    "            params,\n",
    "        )\n",
    "        # Create vector index #CALL db.index.vector.createNodeIndex\n",
    "        try:\n",
    "            graph.query(\n",
    "                \"CALL db.index.vector.createNodeIndex('hypothetical_questions', \"\n",
    "                \"'Question', 'embedding', 768, 'cosine')\",\n",
    "                {},\n",
    "            )\n",
    "        except ClientError:  # already exists\n",
    "            pass\n",
    "#CALL db.index.vector.createNodeIndex('hypothetical_questions', 'Question', 'embedding', 768, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "027f0be0-c90a-4226-93c1-0ac9c057465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ingest summaries\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            (\"Generate a summary of the following input: {question}\\n\" \"Summary:\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f9c4136d-bb5f-43e8-8831-6e737e454e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, parent in enumerate(parent_documents):\n",
    "    summary = summary_chain.invoke({\"question\": parent.page_content}).content\n",
    "    params = {\n",
    "        \"parent_id\": i,\n",
    "        \"summary\": summary,\n",
    "        \"embedding\": EMBEDDING_MODEL.embed_query(summary)# EMBEDDING_MODEL.get_embeddings([summary])[0].values,\n",
    "    }\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    MERGE (p)-[:HAS_SUMMARY]->(s:Summary)\n",
    "    SET s.text = $summary\n",
    "    WITH s\n",
    "    CALL db.create.setVectorProperty(s, 'embedding', $embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index\n",
    "    #CALL db.index.vector.createNodeIndex('typical_rag', 'Parent', 'embedding', 768, 'cosine')\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('summary', \"\n",
    "            \"'Summary', 'embedding', 768, 'cosine')\",\n",
    "            {},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "# CALL db.index.vector.createNodeIndex('summary', 'Summary', 'embedding', 768, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "98144a26-1833-4a29-a4bd-a820b96b0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical RAG retriever\n",
    "EMBEDDING_MODEL_NAME = 'textembedding-gecko@003'\n",
    "\n",
    "\n",
    "\n",
    "typical_rag = Neo4jVector.from_existing_index(\n",
    "    url = url,\n",
    "    username = username,\n",
    "    password = password,\n",
    "    embedding = VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
    "    index_name = 'typical_rag'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "82738c40-dac1-4b6f-a0a8-ea4df04ef336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parent_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_CHILD]-(parent)\n",
    "WITH parent, max(score) AS score // deduplicate parents\n",
    "RETURN parent.text AS text, score, {} AS metadata LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "parent_vectorstore = Neo4jVector.from_existing_index(\n",
    "    url = url,\n",
    "    username = username,\n",
    "    password = password,\n",
    "    embedding = VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
    "    index_name=\"parent_document\",\n",
    "    retrieval_query=parent_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1d276f6b-84a6-4164-8100-b760358fe416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Hypothetic questions retriever\n",
    "\n",
    "hypothetic_question_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_QUESTION]-(parent)\n",
    "WITH parent, max(score) AS score // deduplicate parents\n",
    "RETURN parent.text AS text, score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "hypothetic_question_vectorstore = Neo4jVector.from_existing_index(\n",
    "    url = url,\n",
    "    username = username,\n",
    "    password = password,\n",
    "    embedding = VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
    "    index_name=\"hypothetical_questions\",\n",
    "    retrieval_query=hypothetic_question_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "218ec595-bdf7-4e57-ab0e-4ad6a1ca7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # Summary retriever\n",
    "\n",
    "summary_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_SUMMARY]-(parent)\n",
    "WITH parent, max(score) AS score // deduplicate parents\n",
    "RETURN parent.text AS text, score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "summary_vectorstore = Neo4jVector.from_existing_index(\n",
    "    url = url,\n",
    "    username = username,\n",
    "    password = password,\n",
    "    embedding = VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
    "    index_name=\"summary\",\n",
    "    retrieval_query=summary_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "9b259b4d-843f-42a3-b7ed-900286336555",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = typical_rag.as_retriever().configurable_alternatives(\n",
    "    ConfigurableField(id=\"strategy\"),\n",
    "    default_key=\"typical_rag\",\n",
    "    parent_strategy=parent_vectorstore.as_retriever(),\n",
    "    hypothetical_questions=hypothetic_question_vectorstore.as_retriever(),\n",
    "    summary_strategy=summary_vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c72857d6-3104-4a13-bd73-4950e39aa215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a482d0de-8b04-451a-bb3a-2f0cb75e9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your are a financial analyst Answer the question based only on the following context:\n",
    "{context}\n",
    "Use this above information to answer queries about Environment, Social, and Governance Report\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "4f4a5f17-bb2a-4414-9553-e0e899cd552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-1.0-pro-vision\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    max_retries=6,\n",
    "    stop=None,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "73db2daa-52db-4c8c-bd2a-b0d841931fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "33c78cf9-bd31-4322-8434-82137d990511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add typing for input\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "chain = chain.with_types(input_type=Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b9af9724-222e-4c2a-aabe-3fed2a15b0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ESG Framework is driven by the UN Sustainable Development Goals, the UAE Emirates NBD Vision, and the UN Global Compact Principles. It is reported using the GRI Standards, the Dubai Financial Market ESG Reporting Guide, and the IIRC Framework.\n"
     ]
    }
   ],
   "source": [
    "original_query = \"What is ESG Framework?\"\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\"question\": original_query},\n",
    "        {\"configurable\": {\"strategy\": \"summary_strategy\"}},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "483ac45d-6ed0-45c3-a339-6a57b2cae06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key focus areas of the framework are Environment, Social, and Governance.\n"
     ]
    }
   ],
   "source": [
    "original_query = \"Explain about Key Focus Area?\"\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\"question\": original_query},\n",
    "        {\"configurable\": {\"strategy\": \"parent_strategy\"}},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8b18d626-9d10-426e-8896-bb8828dd7888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emirates Islamic promotes financial literacy and inclusion through initiatives such as:\n",
      "\n",
      "* Financial literacy programs for customers and the community\n",
      "* Partnerships with educational institutions and NGOs\n",
      "* Simplified financial products and services\n",
      "* Digital banking platforms for easy access to financial services\n"
     ]
    }
   ],
   "source": [
    "original_query = \"Summarize Financial Literacy and Inclusion?\"\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\"question\": original_query},\n",
    "        {\"configurable\": {\"strategy\": \"hypothetical_questions\"}},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "cccb06f7-6410-4242-990e-8134e0ef1f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emirates Islamic Bank is committed to expanding financial services for underserved groups. Initiatives include e-learning programs, customer education materials, and disability-friendly branches. The bank also partners with MBRHE to promote financial literacy in schools, universities, and government departments. Additionally, Emirates Islamic supports women entrepreneurs through the Businesswomen Account UAE.\n"
     ]
    }
   ],
   "source": [
    "original_query = \"Summarize Financial Literacy and Inclusion?\"\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\"question\": original_query},\n",
    "        {\"configurable\": {\"strategy\": \"typical_rag\"}},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "ddb0a781-cff1-4241-904d-ac5245785f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback and Coaching: Emirates Islamic fosters a coaching culture to enhance interactions and organizational transformation. This involves providing feedback, identifying learning needs, and developing capabilities to support business strategies.\n"
     ]
    }
   ],
   "source": [
    "original_query = \"Sumaarize Feedback and Coaching:?\"\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\"question\": original_query},\n",
    "        {\"configurable\": {\"strategy\": \"parent_strategy\"}},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8799ca-c403-4c12-ad74-9a2be2ee83ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5518c9f-5f81-4f92-9bf4-e5cc062c40ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf485a9-af43-48a1-8d2f-9a91c58c3e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
